{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "\n",
    "    'train' : DataLoader(train_data,\n",
    "                         batch_size=100,\n",
    "                         shuffle=True,\n",
    "                         num_workers=1\n",
    "    ),\n",
    "\n",
    "    'test' : DataLoader(test_data,\n",
    "                         batch_size=100,\n",
    "                         shuffle=True,\n",
    "                         num_workers=1\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3421849350.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    def_init_(self):\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def_init_(self):\n",
    "        super(CNN, self)._init_()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.view(-1, 320)\n",
    "        x = F.relu(self.fcl(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)  # Corrected to use x.view\n",
    "        x = F.relu(self.fc1(x))  # Corrected self.fcl to self.fc1\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)  # Added dim=1 for softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the appropriate device\n",
    "            output = model(data)  # Get model predictions\n",
    "            test_loss += loss_fn(output, target).item()  # Accumulate loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)  # Calculate average loss\n",
    "\n",
    "    accuracy = 100. * correct / len(loaders['test'].dataset)  # Calculate accuracy\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. *correct / len(loaders[\"test\"].dataset):.0f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303349\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.286038\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.207223\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.000471\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.933598\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.814113\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.808788\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.847966\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.757896\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.727346\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.785586\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.713832\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.686790\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.685197\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.680573\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.591515\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.637545\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.604409\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.625784\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 1.629785\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.603871\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.619546\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.600621\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 1.598011\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.563206\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.632698\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 1.652140\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.563261\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.568316\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 1.619914\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9357/10000 (94%\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.629270\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 1.562737\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 1.559333\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 1.588342\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.619483\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.598507\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1.542757\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 1.600112\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.600704\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 1.583634\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.616270\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 1.606597\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.593873\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 1.567079\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 1.632153\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.651933\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.586190\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 1.617029\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 1.592990\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 1.583899\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.624175\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 1.544243\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 1.621521\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 1.590406\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.598992\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.583023\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 1.615127\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 1.574720\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 1.580039\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 1.549043\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9520/10000 (95%\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.537888\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 1.541487\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 1.599487\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 1.599428\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 1.583951\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.574991\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 1.535092\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 1.553863\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.536211\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 1.544666\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.587870\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 1.599261\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 1.574522\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 1.585050\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 1.539152\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.583916\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.536575\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 1.564162\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 1.573497\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 1.579869\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.550153\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 1.541687\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 1.559443\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 1.551083\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.523149\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.555028\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 1.558856\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 1.547442\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 1.557164\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 1.545298\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9611/10000 (96%\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.533059\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 1.578406\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 1.570399\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 1.553647\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 1.548206\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.516073\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 1.565622\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 1.521295\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 1.558300\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 1.513801\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.536431\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 1.551312\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 1.588148\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 1.505122\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 1.519559\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.502798\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.562365\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 1.551190\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 1.594552\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 1.554157\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.557020\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 1.567437\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 1.534765\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 1.553796\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.524575\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.522141\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 1.585888\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 1.532187\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 1.506304\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 1.528576\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9652/10000 (97%\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.535830\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 1.529896\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 1.486883\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 1.537104\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 1.537695\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.529727\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 1.573022\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 1.539787\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.579724\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 1.544918\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.508437\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 1.566382\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 1.550435\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 1.500121\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 1.535146\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.546172\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.567597\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 1.536900\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 1.496567\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 1.526858\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.559814\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 1.570952\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 1.553504\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 1.573604\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.575273\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.557113\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 1.536427\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 1.537126\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 1.545278\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 1.572718\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9720/10000 (97%\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.533517\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 1.519785\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 1.521931\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 1.516933\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 1.532173\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.541711\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 1.567288\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 1.551358\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 1.555784\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 1.535269\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.529692\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 1.532873\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 1.533027\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 1.509419\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 1.550078\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.542566\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.502799\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 1.581988\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 1.567581\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 1.524236\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.541141\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 1.506763\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 1.504004\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 1.521139\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 1.517277\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.504788\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 1.489571\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 1.544864\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 1.532727\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 1.544274\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9717/10000 (97%\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.537555\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 1.527560\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 1.506960\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 1.512043\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 1.519470\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.549888\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 1.579543\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 1.515269\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 1.518402\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 1.489327\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.506127\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 1.535832\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 1.543533\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 1.537431\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 1.561716\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.502188\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.530499\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 1.504518\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 1.552952\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 1.541417\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.534797\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 1.512573\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 1.493520\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 1.523683\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 1.544253\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.553493\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 1.538868\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 1.512824\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 1.531299\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 1.480200\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9720/10000 (97%\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.556902\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 1.519781\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 1.544664\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 1.577803\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 1.533738\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.538749\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 1.521378\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 1.509195\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 1.517343\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 1.557223\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.523607\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 1.545531\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 1.531108\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 1.551256\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 1.530327\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.491244\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.525729\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 1.531728\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 1.534091\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 1.536505\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.508258\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 1.491889\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 1.498107\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 1.508454\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 1.545863\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.516895\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 1.518083\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 1.487849\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 1.530117\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 1.505265\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9735/10000 (97%\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.541715\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 1.484000\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 1.521997\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 1.558038\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 1.482360\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.501993\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 1.532036\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 1.537394\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 1.551469\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 1.537385\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.557301\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 1.517636\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 1.516364\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 1.533426\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 1.540900\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.530311\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.510727\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 1.510723\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 1.525428\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 1.544982\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.528826\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 1.501716\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 1.502355\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 1.522683\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 1.504941\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.523230\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 1.527054\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 1.530606\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 1.514042\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 1.562156\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9750/10000 (98%\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.547778\n",
      "Train Epoch: 10 [2000/60000 (3%)]\tLoss: 1.572936\n",
      "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 1.487835\n",
      "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 1.580671\n",
      "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 1.576689\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.538943\n",
      "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 1.532471\n",
      "Train Epoch: 10 [14000/60000 (23%)]\tLoss: 1.493491\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.524231\n",
      "Train Epoch: 10 [18000/60000 (30%)]\tLoss: 1.521061\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.519491\n",
      "Train Epoch: 10 [22000/60000 (37%)]\tLoss: 1.508449\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 1.515762\n",
      "Train Epoch: 10 [26000/60000 (43%)]\tLoss: 1.543459\n",
      "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 1.523344\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.473847\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.533783\n",
      "Train Epoch: 10 [34000/60000 (57%)]\tLoss: 1.560788\n",
      "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 1.494656\n",
      "Train Epoch: 10 [38000/60000 (63%)]\tLoss: 1.544295\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.511754\n",
      "Train Epoch: 10 [42000/60000 (70%)]\tLoss: 1.502941\n",
      "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 1.547996\n",
      "Train Epoch: 10 [46000/60000 (77%)]\tLoss: 1.519096\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 1.550061\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.571440\n",
      "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 1.547650\n",
      "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 1.499751\n",
      "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 1.550692\n",
      "Train Epoch: 10 [58000/60000 (97%)]\tLoss: 1.566388\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 9761/10000 (98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicition: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a test data sample (assuming `test_data` is a DataLoader or similar object)\n",
    "data, target = test_data[3]  # Retrieve the first test sample\n",
    "\n",
    "# Prepare the data (unsqueeze to add batch dimension and move to the correct device)\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "# Perform the prediction\n",
    "output = model(data)\n",
    "\n",
    "# Get the predicted class (argmax across the output dimension)\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "print(f'Predicition: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
